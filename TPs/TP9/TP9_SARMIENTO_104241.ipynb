{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ee27f8",
   "metadata": {},
   "source": [
    "# TRABAJO PRÁCTICO 9 - NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "id": "1146d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias\n",
    "\n",
    "#Generales\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "#Descarga del catálogo\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "#Manejo de los epubs\n",
    "import os\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ec4b2",
   "metadata": {},
   "source": [
    "# Procesamiento del catálogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e57c7e",
   "metadata": {},
   "source": [
    "#### Descarga del catálogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "id": "0e881616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descarga de archivos completa.\n",
      "La carpeta 'compressed' ya existe, no se vuelve a extraer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se colocan las URLs de los archivos a descargar\n",
    "url_catalog = \"https://web.csc.gob.ar/~jzuloaga/epub/catalog.csv\"\n",
    "url_tar = \"https://web.csc.gob.ar/~jzuloaga/epub/compressed.tar\"\n",
    "\n",
    "# Se colocan nombres a los archivos\n",
    "file_catalog = \"catalog.csv\"\n",
    "file_tar = \"compressed.tar\"\n",
    "\n",
    "# Se descargan solo si no existen\n",
    "if not os.path.exists(file_catalog):\n",
    "    print(\"Descargando catalog.csv...\")\n",
    "    urllib.request.urlretrieve(url_catalog, file_catalog)\n",
    "\n",
    "if not os.path.exists(file_tar):\n",
    "    print(\"Descargando compressed.tar...\")\n",
    "    urllib.request.urlretrieve(url_tar, file_tar)\n",
    "\n",
    "print(\"Descarga de archivos completa.\")\n",
    "\n",
    "# Descompresion del catalogo\n",
    "# Se descomprimen los archivos\n",
    "if not os.path.exists(\"compressed\"):\n",
    "    with tarfile.open(file_tar, \"r\") as tar:\n",
    "        tar.extractall(\"compressed\")\n",
    "        print(\"Archivos extraídos en la carpeta 'compressed'\\n\")\n",
    "else:\n",
    "    print(\"La carpeta 'compressed' ya existe, no se vuelve a extraer.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08f79b",
   "metadata": {},
   "source": [
    "#### Cargar el catálogo y explorar el contenido de sus columnas.  ¿Qué representa cada una?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "id": "ea9eac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del catálogo:\n",
      "Index(['EPL Id', 'Título', 'Autor', 'Géneros', 'Colección', 'Volumen',\n",
      "       'Año publicación', 'Sinopsis', 'Páginas', 'Revisión', 'Idioma',\n",
      "       'Publicado', 'Estado', 'Valoración', 'Nº Votos', 'Enlace(s)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Se carga el catalogo\n",
    "df_catalog = pd.read_csv(\"catalog.csv\")\n",
    "\n",
    "# Se muestran las columnas del catalogo\n",
    "print(\"Columnas del catálogo:\")\n",
    "print(df_catalog.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37dc58",
   "metadata": {},
   "source": [
    "#### Significado de las columnas\n",
    "\n",
    "EPL Id: Id del libro en el catálogo.\n",
    "\n",
    "Título: Título del libro.\n",
    "\n",
    "Autor: Autor del libro.\n",
    "\n",
    "Géneros: Géneros del libro (una lista de géneros para cada libro).\n",
    "\n",
    "Colección: Colección, serie o saga a la que pertenece el libro.\n",
    "\n",
    "Volumen: Número del volumen del libro dentro de la colección.\n",
    "\n",
    "Año publicación: Año de publicación del libro.\n",
    "\n",
    "Sinopsis: Descripción o resumen del contenido del libro.\n",
    "\n",
    "Páginas: Cantidad de páginas.\n",
    "\n",
    "Revisión: Información editorial o revisión del texto.\n",
    "\n",
    "Idioma: Idioma del libro.\n",
    "\n",
    "Publicado: Estado o fecha de publicación.\n",
    "\n",
    "Estado: Estado del libro dentro del catálogo.\n",
    "\n",
    "Valoración: Valoración promedio de los usuarios.\n",
    "\n",
    "Nº Votos: Cantidad de votos recibidos por los usuarios.\n",
    "\n",
    "Enlace(s): Enlaces relacionados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a48b67",
   "metadata": {},
   "source": [
    "#### Filtrar las entradas del catálogo, de manera de quedarse solamente con los libros en idioma español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "id": "6dc835b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalog = df_catalog[df_catalog['Idioma'] == 'Español']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afe504",
   "metadata": {},
   "source": [
    "#### Limitar las entradas del catálogo a las que tenga su correspondiente libro digital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ebbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de archivos EPUB disponibles: 8958\n"
     ]
    }
   ],
   "source": [
    "path_books = 'compressed/compressed'\n",
    "\n",
    "# Se crea una lista con los epubs en path_books\n",
    "epubs = [f for f in os.listdir(path_books) if f.endswith(\".epub\")]\n",
    "\n",
    "print(f\"Total de archivos EPUB disponibles: {len(epubs)}\")\n",
    "\n",
    "# Se crea un conjunto de ids válidos, es decir, se acumulan los ids de los epubs que existen en la ruta path_books\n",
    "valids_id = set()\n",
    "for f in epubs:\n",
    "    try:\n",
    "        valids_id.add(int(f.replace(\".epub\", \"\")))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Se reduce el catálogo a aquellos libros que tienen su versión digital (es decir, que existe en la carpeta descomprimida)\n",
    "df_catalog = df_catalog[df_catalog['EPL Id'].isin(valids_id)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5468d4",
   "metadata": {},
   "source": [
    "# Definición de las clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8ad0a",
   "metadata": {},
   "source": [
    "#### Analizar la distribución de libros por categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2db753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama         1201\n",
       "Otros         1059\n",
       "Aventuras     1000\n",
       "Policial       998\n",
       "Realista       854\n",
       "Intriga        689\n",
       "Histórico      591\n",
       "Filosofía      553\n",
       "Historia       533\n",
       "Fantástico     470\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1052,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se separan los géneros y se cuentan individualmente\n",
    "# Se busca eliminar los espacios despues de las comas y los posteriores al último caracter de la palabra\n",
    "all_genres = df_catalog['Géneros'].dropna()\\\n",
    "    .str.split(r',\\s*')\\\n",
    "    .apply(lambda lst: [g.strip() for g in lst])\n",
    "\n",
    "# Se cuentan los géneros, utilizando una doble compresión.\n",
    "# se toma cada sublista (es decir, cada lista de géneros para cada libro) en all_genres y a cada una de ellas, se les toma cada uno de los géneros\n",
    "# y se aplanan en una lista, donde Counter cuenta la cantidad de ocurrencias de cada uno de los géneros en la lista total \n",
    "genre_counter = Counter([g for sublist in all_genres for g in sublist])\n",
    "\n",
    "# Se crea un Series de pandas y se ordenan las ocurrencias de cada género, de manera descendente\n",
    "genre_proportion = pd.Series(genre_counter).sort_values(ascending=False)\n",
    "\n",
    "genre_proportion.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e53b67",
   "metadata": {},
   "source": [
    "#### Eliminar el género Otros por ser una categoría redundante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483d2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan los libros con únicamente el género \"Otros\"\n",
    "df_catalog = df_catalog[~df_catalog['Géneros'].str.fullmatch(r'Otros', na=False)]\n",
    "\n",
    "# Se elimina el género \"Otros\" de los demás libros, considerando nuevamente los casos con espacios post comas y post ultimo caracter\n",
    "df_catalog['Géneros'] = (\n",
    "    df_catalog['Géneros']\n",
    "    .str.replace(r',?\\s*Otros', '', regex=True)\n",
    "    .str.strip(', ')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fa072",
   "metadata": {},
   "source": [
    "#### Proponer y justificar un criterio para elegir un único género cuando un libro tenga varios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función para seleccionar el género con menor aparición global para cada libro\n",
    "def pick_rarest_genre(genres):\n",
    "\n",
    "    # Se verifica si genres es NaN\n",
    "    if pd.isna(genres):\n",
    "        return None\n",
    "    \n",
    "    # Se divide la lista de géneros usando las comas como separador.\n",
    "    # strip elimina los espacios al inicio y al final, split los separa en \n",
    "    genre_list = [g.strip() for g in genres.split(',')]\n",
    "\n",
    "    # Se ordena por frecuencia (de menor a mayor) \n",
    "    # Se utiliza genre_counter que es la cantidad de apariciones de los géneros en el df\n",
    "    # Se toma el elemento que tenga la mínima cantidad de operaciones\n",
    "    # La función lambda toma la frecuencia del género, si no se encuentra en el diccionario asume 0\n",
    "    rarest = min(genre_list, key=lambda g: genre_counter.get(g, 0))\n",
    "    \n",
    "    return rarest\n",
    "\n",
    "# Se crea una columna con el género con la menor cantidad de apariciones en el dataframe para cada libro\n",
    "df_catalog['Género_único'] = df_catalog['Géneros'].apply(pick_rarest_genre)\n",
    "\n",
    "# Se elimina la columna \"Géneros\" del catálogo\n",
    "df_catalog.drop(columns=['Géneros'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b32c57",
   "metadata": {},
   "source": [
    "#### Reportar la distribución final de libros por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544e07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Género_único\n",
       "Realista           0.085018\n",
       "Drama              0.081098\n",
       "Policial           0.076320\n",
       "Aventuras          0.074237\n",
       "Intriga            0.073012\n",
       "Histórico          0.058434\n",
       "Historia           0.047287\n",
       "Filosofía          0.044469\n",
       "Fantástico         0.034669\n",
       "Ciencia ficción    0.034179\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 1055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se cuenta la cantidad de apariciones de cada género en la columna de 'Genero_unico'\n",
    "final_distribution = df_catalog['Género_único'].value_counts()\n",
    "\n",
    "final_distribution = final_distribution\n",
    "\n",
    "final_distribution.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83505b75",
   "metadata": {},
   "source": [
    "#### Separar los libros para definir conjuntos de entrenamiento y testeo utilizando las proporciones 75/25. Fijar la semilla para reproducibilidad utilizando su número de padrón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del catálogo completo: (8163, 17)\n",
      "Tamaño del conjunto de entrenamiento: (6122, 17)\n",
      "Tamaño del conjunto de testeo: (2041, 17)\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los géneros únicos\n",
    "unique_genre = df_catalog['Género_único'].unique()\n",
    "\n",
    "# Se genera un diccionario con una etiqueta para cada genero (1 : unique_genre +1)\n",
    "dict_genre = {genero: idx + 1 for idx, genero in enumerate(unique_genre)}\n",
    "\n",
    "# Se cargan las etiquetas para cada género en el df\n",
    "df_catalog['Etiqueta_género'] = df_catalog['Género_único'].map(dict_genre)\n",
    "\n",
    "# Se cargan en X los datos del catálogo, son seleccionados sin incluir las etiquetas ni su género\n",
    "X = df_catalog\n",
    "\n",
    "# Se cargan en y los datos a predecir\n",
    "y = df_catalog[\"Etiqueta_género\"]\n",
    "\n",
    "# Número de padrón\n",
    "student_id = 104241\n",
    "\n",
    "# Se separan en datos de entrenamiento y testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    random_state=student_id,\n",
    ")\n",
    "\n",
    "print('Tamaño del catálogo completo:', df_catalog.shape)\n",
    "print('Tamaño del conjunto de entrenamiento:', X_train.shape)\n",
    "print('Tamaño del conjunto de testeo:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24050d",
   "metadata": {},
   "source": [
    "# Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426b651",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "00bc6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_books = 'compressed/compressed'\n",
    "\n",
    "def get_text_epub(epub_path):\n",
    "\n",
    "    try:\n",
    "        # Se abre el archivo epub\n",
    "        book = epub.read_epub(epub_path)\n",
    "        \n",
    "        chapters_list = []\n",
    "\n",
    "        # Se recorren todos los documentos XHTML del libro\n",
    "        for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "            \n",
    "            # Se obtiene el contenido del documento XHTML\n",
    "            xhtml_content = item.get_content()\n",
    "            \n",
    "            # Se usa BeautifulSoup para limpiar el xhtml y solo obtener el texto\n",
    "            soup = BeautifulSoup(xhtml_content, 'html.parser')\n",
    "            \n",
    "            # Se limpia el texto extraído\n",
    "            clean_text = soup.get_text()\n",
    "            \n",
    "            chapters_list.append(clean_text)\n",
    "        \n",
    "        # Se unifican los capitulos en un string\n",
    "        return \" \".join(chapters_list)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No se encontró el archivo: {epub_path}\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando (corrupto): {epub_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def get_text_epub_id (id_book):\n",
    "    \n",
    "    # Se construye la ruta completa al libro epub\n",
    "    filename = f\"{id_book}.epub\"\n",
    "    \n",
    "    epub_path = os.path.join(path_books, filename)\n",
    "\n",
    "    return get_text_epub(epub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cd575",
   "metadata": {},
   "source": [
    "#### El formato de libros epub es un archivo comprimido zip que contiene la metadata y estructura del libro, archivos multimedia y archivos xhtml con el texto del libro. Extraer el texto de esos archivos. Podrá realizarlo manualmente o valerse de bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "id": "99c820d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libros en X_train: 6122\n",
      "Libros en X_test: 2041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ebooklib\\utils.py:36\u001b[39m, in \u001b[36mparse_string\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     tree = etree.parse(io.BytesIO(\u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)), parser=parser)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'bytes' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1058]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLibros en X_test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Se cargan en X_train y X_test los textos de cada uno de los libros, utilizando su ID\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m X_train.loc[:, \u001b[33m'\u001b[39m\u001b[33mtexto\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEPL Id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_text_epub_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m X_test.loc[:, \u001b[33m'\u001b[39m\u001b[33mtexto\u001b[39m\u001b[33m'\u001b[39m] = X_test[\u001b[33m'\u001b[39m\u001b[33mEPL Id\u001b[39m\u001b[33m'\u001b[39m].apply(get_text_epub_id)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSe completó la extracción de los textos\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1057]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mget_text_epub_id\u001b[39m\u001b[34m(id_book)\u001b[39m\n\u001b[32m     41\u001b[39m filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mid_book\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.epub\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m epub_path = os.path.join(path_books, filename)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_text_epub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepub_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1057]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_text_epub\u001b[39m\u001b[34m(epub_path)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Se recorren todos los documentos XHTML del libro\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# Se obtiene el contenido del documento XHTML\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     xhtml_content = \u001b[43mitem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Se usa BeautifulSoup para limpiar el xhtml y solo obtener el texto\u001b[39;00m\n\u001b[32m     18\u001b[39m     soup = BeautifulSoup(xhtml_content, \u001b[33m'\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ebooklib\\epub.py:421\u001b[39m, in \u001b[36mEpubHtml.get_content\u001b[39m\u001b[34m(self, default)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_content\u001b[39m(\u001b[38;5;28mself\u001b[39m, default=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    410\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    411\u001b[39m \u001b[33;03m    Returns content for this document as HTML string. Content will be of type 'str' (Python 2)\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[33;03m    or 'bytes' (Python 3).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    418\u001b[39m \u001b[33;03m      Returns content of this document.\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     tree = \u001b[43mparse_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_template\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_template_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m     tree_root = tree.getroot()\n\u001b[32m    424\u001b[39m     tree_root.set(\u001b[33m\"\u001b[39m\u001b[33mlang\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.lang \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.book.language)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\solek\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\ebooklib\\utils.py:38\u001b[39m, in \u001b[36mparse_string\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     36\u001b[39m     tree = etree.parse(io.BytesIO(s.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)), parser=parser)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     tree = \u001b[43metree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# n_samples = 100\n",
    "\n",
    "# # Se toman unas pocas muestras para evitar ejecutar con todos los archivos juntos. Se toman por índices para conservar alineados los datos.\n",
    "# X_train = X_train.iloc[:n_samples].copy()\n",
    "# y_train = y_train.iloc[:n_samples].copy()\n",
    "# X_test = X_test.iloc[:n_samples].copy()\n",
    "# y_test = y_test.iloc[:n_samples].copy()\n",
    "\n",
    "print(f\"Libros en X_train: {len(X_train)}\")\n",
    "print(f\"Libros en X_test: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# Se cargan en X_train y X_test los textos de cada uno de los libros, utilizando su ID\n",
    "X_train.loc[:, 'texto'] = X_train['EPL Id'].apply(get_text_epub_id)\n",
    "X_test.loc[:, 'texto'] = X_test['EPL Id'].apply(get_text_epub_id)\n",
    "\n",
    "print(\"Se completó la extracción de los textos\")\n",
    "\n",
    "# Se eliminan las filas que no tienen texto (vacías o con NaN)\n",
    "X_train_clean = X_train.dropna(subset=['texto'])\n",
    "X_test_clean = X_test.dropna(subset=['texto'])\n",
    "\n",
    "# Se seleccionan las etiquetas que corresponden a libros cuyo texto no está vacío\n",
    "y_train_clean = y_train.loc[X_train_clean.index]\n",
    "y_test_clean = y_test.loc[X_test_clean.index]\n",
    "\n",
    "print('Datos válidos para X_train:', X_train_clean.shape)\n",
    "print('Datos válidos para X_test:', X_test_clean.shape)\n",
    "\n",
    "print('Datos válidos para y_train:', y_train_clean.shape)\n",
    "print('Datos válidos para y_test:', y_test_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdb1d0",
   "metadata": {},
   "source": [
    "#### Aplicar CountVectorizer (sklearn) al texto. Ajustar max_df, min_df y stop_words a criterio personal, justificando las decisiones. El corpus de texto completo es demasiado extenso para la memoria. Se sugiere el uso de Generators para procesar el texto plano on-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ee14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\solek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Se declara el generator, para procesar el texto por partes\n",
    "def text_generator(df_clean):\n",
    "    for texto in df_clean['texto']:\n",
    "        yield texto\n",
    "\n",
    "# Se descargan las stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Se definen las stopwords en español\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Se crea el vectorizador utilizando las stopwords en español (Palabras que se ignoran del texto completo).\n",
    "# Se cuentan las apariciones de las palabras en los textos, \n",
    "# siendo guardados en una matriz de shape: (n_libros, n_palabras_del_vocabulario) cada posición, almacena esa cantidad de apariciones\n",
    "# Se seleccionan los porcentajes para min_df y max_df ya que se considera que palabras demasiado\n",
    "# poco frecuentes o demasiado frecuentes en el total de los libros, no aportan información significativa para distinguir entre\n",
    "# un género y otro.\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=spanish_stopwords,\n",
    "    max_df=0.9,    # Se descartan las palabras más frecuentes, las que aparecen en más del 90% de los libros\n",
    "    min_df= 0.01   # Se descartan las palabras que aparecen en menos del 1% de los libros\n",
    ")\n",
    "\n",
    "\n",
    "# Se entrena el vectorizador con todos los textos de los libros\n",
    "train_gen = list(text_generator(X_train_clean))\n",
    "X_train_vec = vectorizer.fit_transform(train_gen)\n",
    "\n",
    "test_gen = list(text_generator(X_test_clean))\n",
    "X_test_vec = vectorizer.transform(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f78656",
   "metadata": {},
   "source": [
    "#### Se desea comprobar el funcionamiento del vectorizador. Para ello, aplicar el vectorizador ya entrenado a dos obras clásicas del catálogo de diferentes géneros (por ejemplo, “Estudio en escarlata” y “Orgullo y prejuicio”). Descartar las palabras presentes en ambos libros. Luego, para cada libro, reportar las 40 palabras más frecuentes. Interpretar los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más frecuentes en 'Alicia en el país de las maravillas' (excluyendo las que se comparten):\n",
      "alicia -> 433\n",
      "tortuga -> 64\n",
      "grifo -> 55\n",
      "conejo -> 53\n",
      "lirón -> 39\n",
      "gustaría -> 20\n",
      "bebé -> 14\n",
      "corten -> 13\n",
      "cocinera -> 13\n",
      "mantequilla -> 11\n",
      "enseguida -> 10\n",
      "jardineros -> 10\n",
      "croquet -> 10\n",
      "moraleja -> 10\n",
      "cheshire -> 9\n",
      "sonrisa -> 8\n",
      "pizarras -> 8\n",
      "melaza -> 8\n",
      "cogió -> 8\n",
      "centímetros -> 7\n",
      "pizca -> 7\n",
      "temblorosa -> 7\n",
      "lagartija -> 7\n",
      "nadar -> 7\n",
      "ansiedad -> 7\n",
      "debería -> 6\n",
      "cerdito -> 6\n",
      "chilló -> 6\n",
      "señorita -> 6\n",
      "carroll -> 6\n",
      "enfadado -> 6\n",
      "barbilla -> 6\n",
      "puertecita -> 5\n",
      "pipa -> 5\n",
      "cambiado -> 5\n",
      "dedal -> 5\n",
      "gruñido -> 5\n",
      "medía -> 5\n",
      "intrascendente -> 5\n",
      "lío -> 5\n",
      "\n",
      "Palabras más frecuentes en 'Historia de los heterodoxos españoles' (excluyendo las que se comparten):\n",
      "et -> 2822\n",
      "san -> 1633\n",
      "españa -> 1510\n",
      "juan -> 1226\n",
      "iglesia -> 1197\n",
      "siglo -> 1033\n",
      "non -> 846\n",
      "fe -> 801\n",
      "fr -> 786\n",
      "doctrina -> 740\n",
      "obispo -> 720\n",
      "espíritu -> 689\n",
      "santo -> 685\n",
      "pedro -> 679\n",
      "etc -> 645\n",
      "ad -> 625\n",
      "madrid -> 620\n",
      "inquisición -> 527\n",
      "filosofía -> 521\n",
      "cf -> 519\n",
      "españoles -> 503\n",
      "valdés -> 503\n",
      "qui -> 502\n",
      "quod -> 485\n",
      "cristo -> 474\n",
      "erasmo -> 473\n",
      "ut -> 465\n",
      "religión -> 460\n",
      "páginas -> 453\n",
      "ii -> 436\n",
      "biblioteca -> 432\n",
      "obispos -> 432\n",
      "ciencia -> 426\n",
      "francisco -> 424\n",
      "concilio -> 416\n",
      "sevilla -> 409\n",
      "per -> 408\n",
      "ley -> 397\n",
      "antonio -> 396\n",
      "carlos -> 395\n",
      "Entre los tops de palabras más frecuentes, se comparten: set()\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los textos de 2 libros diferentes\n",
    "text_alice = X_train_clean.loc[X_train_clean['Título'] == 'Alicia en el país de las maravillas (il. de Marta Gómez-Pintado)', 'texto'].iloc[0]\n",
    "text_spain   = X_train_clean.loc[X_train_clean['Título'] == 'Historia de los heterodoxos españoles', 'texto'].iloc[0]\n",
    "\n",
    "\n",
    "# Se vectorizan los textos de cada libro (shapes: 1xn_vocabulario)\n",
    "X_alice = vectorizer.transform([text_alice])\n",
    "X_spain   = vectorizer.transform([text_spain])\n",
    "\n",
    "\n",
    "freq_alice = np.squeeze(np.asarray(X_alice.toarray()))\n",
    "freq_spain   = np.squeeze(np.asarray(X_spain.toarray()))\n",
    "\n",
    "# Se obtienen los índices donde realmente hay palabras\n",
    "idx_alice = set(np.where(freq_alice > 0)[0])\n",
    "idx_spain   = set(np.where(freq_spain > 0)[0])\n",
    "\n",
    "# Se obtienen las palabras en común\n",
    "common_words = idx_alice.intersection(idx_spain)\n",
    "\n",
    "# Se crean listas de las palabras únicas en cada libro\n",
    "words_alice_only = list(idx_alice - common_words)\n",
    "words_spain_only   = list(idx_spain - common_words)\n",
    "\n",
    "# Se obtiene el vocabulario en el vectorizador\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Se obtiene la cantidad de apariciones de cada palabra en cada libro\n",
    "freq_alice_filtered = [(vocabulary[i], freq_alice[i]) for i in words_alice_only]\n",
    "top_alice = sorted(freq_alice_filtered, key=lambda x: x[1], reverse=True)[:40]\n",
    "\n",
    "freq_spain_filtered = [(vocabulary[i], freq_spain[i]) for i in words_spain_only]\n",
    "top_spain = sorted(freq_spain_filtered, key=lambda x: x[1], reverse=True)[:40]\n",
    "\n",
    "\n",
    "print(\"Palabras más frecuentes en 'Alicia en el país de las maravillas' (excluyendo las que se comparten):\")\n",
    "for word, freq in top_alice:\n",
    "        print(word, '->', freq)\n",
    "\n",
    "print(\"\\nPalabras más frecuentes en 'Historia de los heterodoxos españoles' (excluyendo las que se comparten):\")\n",
    "for word, freq in top_spain:\n",
    "        print(word, '->', freq)\n",
    "\n",
    "\n",
    "shared = set(top_alice).intersection(top_spain)\n",
    "print('Entre los tops de palabras más frecuentes, se comparten:', shared)\n",
    "\n",
    "# print(X_alice.shape)\n",
    "# print(X_spain.shape)\n",
    "# print(vocabulary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bcb862",
   "metadata": {},
   "source": [
    "Dado que se eligieron 2 libros de temáticas muy diferentes, es esperable que ninguno de los libros compartan palabras en su top de 40 palabras más usadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e81e7",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7a5af",
   "metadata": {},
   "source": [
    "#### Sobre Naive Bayes (como clasificador): \n",
    "\n",
    "Es un clasificador probabilístico, que busca estimar la probabilidad de que un dato $x$ pertenezca a una clase $y$ y clasifica según qué clase tiene mayor probabilidad.\n",
    "\n",
    "En este caso, los datos son el texto de los libros, mientras que las clases, son los géneros de cada libro. Entonces, se busca que, dado un texto de un libro, se pueda predecir a qué género pertenece.\n",
    "\n",
    "#### Sobre la hipótesis Naive:\n",
    "\n",
    "La hipótesis Naive se basa en asumir que todas las palabras son independientes entre sí, dado que ya conocemos la clase de cada una.\n",
    "Si, por ejemplo, tenemos el libro \"Alicia en el país de las maravillas\", la aparición de \"conejo\" y \"alicia\" es independiente entre sí dado el género.\n",
    "Lo cual, no es estrictamente cierto.\n",
    "\n",
    "#### Sobre Naive Bayes Multinomial:\n",
    "\n",
    "En este modelo, la probabilidad asignada a cada clase (género del libro), dado que se tiene la muestra x (el texto del libro en este caso), es:\n",
    "\n",
    "$$\n",
    "p(y \\mid x) \\propto p(y) \\prod_{i=1}^{n} p(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "Esto nace del teorema de Bayes:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y) \\, P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$y$ es la clase, en este caso el género del libro.\n",
    "\n",
    "x es el texto de un libro.\n",
    "\n",
    "$P(y \\mid x)$ Es la probabilidad de que el libro sea de la clase $y$ dado el texto del libro (lo que se busca predecir).\n",
    "\n",
    "$P(y)$ Es la probabilidad a priori de la clase $y$ (en este caso, la proporción de libros de cada género).\n",
    "\n",
    "$P(x)$ Es la probabilidad marginal de observar esas palabras.\n",
    "\n",
    "$P(x \\mid y)$ Es la probabilidad de observar las palabras x, dado que el libro es del género $y$.\n",
    "\n",
    "La hipótesis Naive, entra a la hora de calcular $P(x \\mid y)$, ya que, dado que las palabras $x_i$ se consideran independientes dado el género al que pertenece el texto del libro, entonces:\n",
    "\n",
    "$$\n",
    "P(x \\mid y) = \\prod_{i=1}^{n} P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "\n",
    "Para el caso de la multinomial, esta probabilidad (sin considerar la marginal de x) puede escribirse como:\n",
    "\n",
    "$p( y = k | x ) \\propto c_y \\cdot \\prod_{j=1}^{V}(\\theta_{j}^{(k)})^{N_j}$ \n",
    "\n",
    "Donde:\n",
    "\n",
    "$y$ es la clase, en este caso el género del libro.\n",
    "\n",
    "x es el texto de un libro, que contiene d palabras: x=($x_1$, $x_2$, ... , $x_d$).\n",
    "\n",
    "$c_y$ es la probabilidad a priori de cada clase (es decir, la cantidad de líbros de un género con respecto al total de libros).\n",
    "\n",
    "$\\theta_j$ son las probabilidades de que se encuentre en el texto la palabra $j$ ($j$ va desde 1 hasta $V$, es decir, abarca todo el vocabulario), dado el género del libro es $y$\n",
    "\n",
    "$N_j$ es la cantidad de veces que aparece la palabra j en el libro.\n",
    "\n",
    "\n",
    "Luego, dado que las probabilidades pueden volverse muy pequeñas y causar inestabilidad numérica, se trabaja con el logaritmo de la probabilidad:\n",
    "\n",
    "$log (p( y = k | x )) = cte + log(c_y) +\\sum_{j=1}^{V}(N_j \\cdot log(\\theta_{j}^{k})) $\n",
    "\n",
    "\n",
    "\n",
    "Entrenamiento del modelo:\n",
    "\n",
    "Para el entrenamiento del modelo, se comienza calculando las probabilidades a priori de cada clase, es decir, la cantidad de libros cada género con respecto al total de libros:\n",
    "\n",
    "$$\n",
    "c_k = p(y=k) = \\frac{\\#\\{y_i=k\\}}{n}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "$k$ es la clase o género\n",
    "\n",
    "$i$ es el i-ésimo libro \n",
    "\n",
    "$y_i$ es el género del i-ésimo libro\n",
    "\n",
    "$n$ es el total de libros\n",
    "\n",
    "luego, se calcula el logaritmo de esta probabilidad, para preparar el cálculo de predict_proba\n",
    "\n",
    "el paso siguiente, consiste en calcular $\\theta$ :\n",
    "\n",
    "$$\n",
    "\\theta^{k}_{j}\n",
    "=\n",
    "\\frac{N_{kj} + \\alpha_{j}}\n",
    "{\\sum_{m=1}^{V} (N_{km} + \\alpha_{m})}\n",
    "$$\n",
    "\n",
    "y el logaritmo del mismo, para preparar los cálculos de la predicción soft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a46ea",
   "metadata": {},
   "source": [
    "#### Utilizando solamente numpy y scipy, implementar el clasificador MNB. El mismo debe contener los métodos fit, predict y predict_proba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5565e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNB:\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.classes = None\n",
    "        self.class_count = None\n",
    "        self.class_log_prior = None\n",
    "        self.words_per_genre = None\n",
    "        self.log_theta = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Entrenamiento\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Se convierte la entrada en un array de numpy\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Se guardan las clases utilizando los identificadores de los géneros\n",
    "        self.classes = np.unique(y)\n",
    "\n",
    "        # Se guarda la cantidad de clases existentes (cantidad de géneros)\n",
    "        K = len(self.classes)\n",
    "\n",
    "        # Se carga N como al cantidad de libros que están vectorizados y V como la cantidad de palabras del vocabulario\n",
    "        N = X.shape[0]\n",
    "        V = X.shape[1]\n",
    "\n",
    "        # Se calculan cuantos libros pertenecen a cada género\n",
    "        self.class_count = np.array([(y == c).sum() for c in self.classes], dtype=float)\n",
    "\n",
    "        # Se calcula el logaritmo de la probabilidad a priori: log (# libros por género / # libros totales) (para evitar inestabilidad numérica)\n",
    "        self.class_log_prior = np.log(self.class_count / N)\n",
    "\n",
    "        # Se crea un vector de ceros de K x V, para contar cuantas veces aparece cada palabra dentro de cada clase\n",
    "        # Es decir, se cuenta cuantas veces aparece una palabra en un libro de un determinado género\n",
    "        self.words_per_genre = np.zeros((K, V), dtype=float)\n",
    "\n",
    "        # Se realiza la cuenta de las palabras presentes en cada género para todos los géneros    \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            self.words_per_genre[idx] = X[y == c].sum(axis=0)\n",
    "\n",
    "        # Se genera el vector de alphas: si es un escalar se llena el array de V alphas con el valor dado\n",
    "        # en caso de que sea una lista (o similares), se lo convierte a array\n",
    "        if np.isscalar(self.alpha):\n",
    "            alpha_vec = np.full(V, self.alpha)\n",
    "        else:\n",
    "            alpha_vec = np.asarray(self.alpha)\n",
    "\n",
    "        # Se calcula:  N_kj + alpha_j\n",
    "        numerator = self.words_per_genre + alpha_vec  # shape (K, V)\n",
    "\n",
    "        # Se calcula: sum(N_km + alpha_m) (m va entre 1 y V)\n",
    "        denominator = numerator.sum(axis=1, keepdims=True)  # (K, 1)\n",
    "\n",
    "        # Se calculan los thetas (probabilidades de que aparezcan  cada una de las palabras del vocabulario dada cada una de las clases)\n",
    "        theta_hat = numerator / denominator\n",
    "\n",
    "        # Se calculan las log-probabilidades para los thetas\n",
    "        self.log_theta = np.log(theta_hat)\n",
    "\n",
    "\n",
    "        print('log_theta', self.log_theta.shape)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predicción soft\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        # Dado que cada posición de la vectorización del textos es: X[i, m] = N_m\n",
    "        # Entonces, se usa el producto matricial @ para el cálculo\n",
    "        # Además, se suma el logaritmo de la probabilidad a priori\n",
    "        # todo esto para calcular el logaritmo de la probabilidad de un libro pertenezca a un determinado género, dado el texto del mismo\n",
    "        log_prob = X @ self.log_theta.T + self.class_log_prior\n",
    "\n",
    "        # Se normaliza log_prob, evitando inestabilidad numérica\n",
    "        log_norm = logsumexp(log_prob, axis=1, keepdims=True)\n",
    "\n",
    "        # Se vuelve al espacio de probabilidades finalmente\n",
    "        return np.exp(log_prob - log_norm)\n",
    "\n",
    "    # Predicción hard\n",
    "    def predict(self, X):\n",
    "        return self.classes[np.argmax(self.predict_proba(X), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dacce3",
   "metadata": {},
   "source": [
    " #### Reportar el Accuracy y el Macro F1, tanto para entrenamiento como testeo. ¿Cuál sería la probabilidad de error asociada a un clasificador dummy en esta tarea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c87c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_theta (58, 91665)\n",
      "Métricas del MNB\n",
      "Accuracy (train): 76.075%\n",
      "Accuracy (test): 49.433%\n",
      "Macro F1 (train): 83.786%\n",
      "Macro F1 (test): 35.814%\n",
      "Métricas Dummy Classifier\n",
      "Accuracy (train): 8.582%\n",
      "Accuracy (test): 8.132%\n",
      "Macro F1 (train): 0.273%\n",
      "Macro F1 (test): 0.284%\n"
     ]
    }
   ],
   "source": [
    "model = MNB(alpha = 0.1)\n",
    "\n",
    "model.fit(X_train_vec,y_train_clean)\n",
    "\n",
    "# Se obtienen las predicciones del modelo\n",
    "y_pred_train = model.predict(X_train_vec)\n",
    "y_pred_test  = model.predict(X_test_vec)\n",
    "\n",
    "# Se obtienen los accuracy\n",
    "acc_train = accuracy_score(y_train_clean, y_pred_train)\n",
    "acc_test  = accuracy_score(y_test_clean, y_pred_test)\n",
    "\n",
    "# Se obtienen los Macro F1\n",
    "f1_train = f1_score(y_train_clean, y_pred_train, average='macro')\n",
    "f1_test  = f1_score(y_test_clean, y_pred_test, average='macro')\n",
    "\n",
    "print('Métricas del MNB')\n",
    "print(f'Accuracy (train): {round(acc_train *100,3)}%' )\n",
    "print(f'Accuracy (test): {round(acc_test *100,3)}%')\n",
    "print(f'Macro F1 (train): {round(f1_train*100,3)}%')\n",
    "print(f'Macro F1 (test): {round(f1_test*100,3)}%')\n",
    "\n",
    "####### clasificador dummy\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Entrenamiento\n",
    "dummy.fit(X_train_vec, y_train_clean)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = dummy.predict(X_train_vec)\n",
    "y_pred_test  = dummy.predict(X_test_vec)\n",
    "\n",
    "# Métricas\n",
    "acc_train = accuracy_score(y_train_clean, y_pred_train)\n",
    "acc_test  = accuracy_score(y_test_clean, y_pred_test)\n",
    "f1_train = f1_score(y_train_clean, y_pred_train, average='macro')\n",
    "f1_test  = f1_score(y_test_clean, y_pred_test, average='macro')\n",
    "\n",
    "print('\\n\\nMétricas Dummy Classifier')\n",
    "print(f'Accuracy (train): {round(acc_train *100,3)}%' )\n",
    "print(f'Accuracy (test): {round(acc_test *100,3)}%')\n",
    "print(f'Macro F1 (train): {round(f1_train*100,3)}%')\n",
    "print(f'Macro F1 (test): {round(f1_test*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab3bfe",
   "metadata": {},
   "source": [
    "Se evidencia que el modelo realiza una mejor predicción que el clasificador dummy, el cual escoje el género más frecuente.\n",
    "\n",
    "Por otro lado, se puede notar que, si se analizan los datos de testeo, el clasificador MNB tiene las clases ligeramente desbalanceadas, ya que el accuracy y el macro-f1 no coinciden. El accuracy, mide los aciertos con respecto al total de predicciones, mientras que el macro-f1, combina cuantas predicciones de una clase eran correctas y de todos los casos reales (es decir, de todos los libros de una clase) cuales se predijeron correctamente. \n",
    "\n",
    "Por esta razón, es evidente que hay géneros que se predicen peor que otros, dado que se ve afectado por las clases con menos aciertos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2617de7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Título</th>\n",
       "      <th>Género real</th>\n",
       "      <th>Género predicho</th>\n",
       "      <th>Valoración</th>\n",
       "      <th>Nº Votos</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>1984</td>\n",
       "      <td>Ciencia ficción</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1021</td>\n",
       "      <td>9086.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26330</th>\n",
       "      <td>El Hobbit</td>\n",
       "      <td>Fantástico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.9</td>\n",
       "      <td>895</td>\n",
       "      <td>7965.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>Un mundo feliz (trad. Ramón Hernández)</td>\n",
       "      <td>Filosófico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.4</td>\n",
       "      <td>625</td>\n",
       "      <td>5250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38568</th>\n",
       "      <td>El conde de Montecristo</td>\n",
       "      <td>Aventuras</td>\n",
       "      <td>Realista</td>\n",
       "      <td>9.3</td>\n",
       "      <td>397</td>\n",
       "      <td>3692.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27191</th>\n",
       "      <td>El retrato de Dorian Gray</td>\n",
       "      <td>Terror</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.6</td>\n",
       "      <td>357</td>\n",
       "      <td>3070.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>El extranjero</td>\n",
       "      <td>Filosófico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.4</td>\n",
       "      <td>312</td>\n",
       "      <td>2620.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Soy leyenda</td>\n",
       "      <td>Terror</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.5</td>\n",
       "      <td>294</td>\n",
       "      <td>2499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>Don Quijote de la Mancha (IV CENTENARIO)</td>\n",
       "      <td>Aventuras</td>\n",
       "      <td>Realista</td>\n",
       "      <td>9.4</td>\n",
       "      <td>256</td>\n",
       "      <td>2406.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11617</th>\n",
       "      <td>El lobo estepario</td>\n",
       "      <td>Filosófico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.7</td>\n",
       "      <td>271</td>\n",
       "      <td>2357.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46891</th>\n",
       "      <td>La isla del tesoro</td>\n",
       "      <td>Aventuras</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.9</td>\n",
       "      <td>258</td>\n",
       "      <td>2296.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Título      Género real  \\\n",
       "12092                                      1984  Ciencia ficción   \n",
       "26330                                 El Hobbit       Fantástico   \n",
       "17099    Un mundo feliz (trad. Ramón Hernández)       Filosófico   \n",
       "38568                   El conde de Montecristo        Aventuras   \n",
       "27191                 El retrato de Dorian Gray           Terror   \n",
       "9883                              El extranjero       Filosófico   \n",
       "994                                 Soy leyenda           Terror   \n",
       "1584   Don Quijote de la Mancha (IV CENTENARIO)        Aventuras   \n",
       "11617                         El lobo estepario       Filosófico   \n",
       "46891                        La isla del tesoro        Aventuras   \n",
       "\n",
       "      Género predicho  Valoración  Nº Votos   score  \n",
       "12092        Realista         8.9      1021  9086.9  \n",
       "26330        Realista         8.9       895  7965.5  \n",
       "17099        Realista         8.4       625  5250.0  \n",
       "38568        Realista         9.3       397  3692.1  \n",
       "27191        Realista         8.6       357  3070.2  \n",
       "9883         Realista         8.4       312  2620.8  \n",
       "994          Realista         8.5       294  2499.0  \n",
       "1584         Realista         9.4       256  2406.4  \n",
       "11617        Realista         8.7       271  2357.7  \n",
       "46891        Realista         8.9       258  2296.2  "
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se genera un dataset de testeo para trabajar cómodamente\n",
    "df_test_clean = X_test_clean.copy()\n",
    "df_test_clean[\"y_true\"] = y_test_clean\n",
    "df_test_clean[\"y_pred_test\"] = y_pred_test\n",
    "\n",
    "# Se añade la columna de texto del género\n",
    "df_test_clean[\"Género real\"] = df_test_clean[\"Género_único\"] \n",
    "df_test_clean[\"Género predicho\"] = df_test_clean[\"y_pred_test\"].map({v:k for k,v in dict_genre.items()})\n",
    "\n",
    "# Se crea una columna con booleanos, para determinar si la predicción fue correcta (True = coinciden)\n",
    "df_test_clean[\"es_correcta\"] = (df_test_clean[\"y_true\"] == df_test_clean[\"y_pred_test\"])\n",
    "\n",
    "# Se seleccionan solo los errores\n",
    "errors = df_test_clean[df_test_clean[\"es_correcta\"] == False].copy()\n",
    "\n",
    "# Se crea una columna con el score, considerando la cantidad de votos y la valoración promedio\n",
    "errors[\"score\"] = errors[\"Valoración\"] * errors[\"Nº Votos\"]\n",
    "\n",
    "\n",
    "# Se ordena por este score de mayor a menor y tomar los del top 10\n",
    "errors_rank10 = errors.sort_values(\"score\", ascending=False).head(10)\n",
    "\n",
    "# Se muestra el top10 de los errores más valorado\n",
    "errors_rank10[[\"Título\", \"Género real\", \"Género predicho\", \"Valoración\", \"Nº Votos\", \"score\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
