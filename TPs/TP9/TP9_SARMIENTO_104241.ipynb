{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ee27f8",
   "metadata": {},
   "source": [
    "# TRABAJO PRÁCTICO 9 - NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1146d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias\n",
    "\n",
    "#Generales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "#Descarga del catálogo\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "#Manejo de los epubs\n",
    "import os\n",
    "import ebooklib\n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "#Stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757ec4b2",
   "metadata": {},
   "source": [
    "# Procesamiento del catálogo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e57c7e",
   "metadata": {},
   "source": [
    "#### Descarga del catálogo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0e881616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descarga de archivos completa.\n",
      "La carpeta 'compressed' ya existe, no se vuelve a extraer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Se colocan las URLs de los archivos a descargar\n",
    "url_catalog = \"https://web.csc.gob.ar/~jzuloaga/epub/catalog.csv\"\n",
    "url_tar = \"https://web.csc.gob.ar/~jzuloaga/epub/compressed.tar\"\n",
    "\n",
    "# Se colocan nombres a los archivos\n",
    "file_catalog = \"catalog.csv\"\n",
    "file_tar = \"compressed.tar\"\n",
    "\n",
    "# Se descargan solo si no existen\n",
    "if not os.path.exists(file_catalog):\n",
    "    print(\"Descargando catalog.csv...\")\n",
    "    urllib.request.urlretrieve(url_catalog, file_catalog)\n",
    "\n",
    "if not os.path.exists(file_tar):\n",
    "    print(\"Descargando compressed.tar...\")\n",
    "    urllib.request.urlretrieve(url_tar, file_tar)\n",
    "\n",
    "print(\"Descarga de archivos completa.\")\n",
    "\n",
    "# Se descomprimen los libros en el archivo .tar si no están descomprimidos\n",
    "if not os.path.exists(\"compressed\"):\n",
    "    with tarfile.open(file_tar, \"r\") as tar:\n",
    "        tar.extractall(\"compressed\")\n",
    "        print(\"Archivos extraídos en la carpeta 'compressed'\\n\")\n",
    "else:\n",
    "    print(\"La carpeta 'compressed' ya existe, no se vuelve a extraer.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce08f79b",
   "metadata": {},
   "source": [
    "#### Cargar el catálogo y explorar el contenido de sus columnas.  ¿Qué representa cada una?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ea9eac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas del catálogo:\n",
      "Index(['EPL Id', 'Título', 'Autor', 'Géneros', 'Colección', 'Volumen',\n",
      "       'Año publicación', 'Sinopsis', 'Páginas', 'Revisión', 'Idioma',\n",
      "       'Publicado', 'Estado', 'Valoración', 'Nº Votos', 'Enlace(s)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Se carga el catalogo\n",
    "df_catalog = pd.read_csv(\"catalog.csv\")\n",
    "\n",
    "# Se muestran las columnas del catalogo\n",
    "print(\"Columnas del catálogo:\")\n",
    "print(df_catalog.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37dc58",
   "metadata": {},
   "source": [
    "#### Significado de las columnas\n",
    "\n",
    "EPL Id: Id del libro en el catálogo.\n",
    "\n",
    "Título: Título del libro.\n",
    "\n",
    "Autor: Autor del libro.\n",
    "\n",
    "Géneros: Géneros del libro (una lista de géneros para cada libro).\n",
    "\n",
    "Colección: Colección, serie o saga a la que pertenece el libro.\n",
    "\n",
    "Volumen: Número del volumen del libro dentro de la colección.\n",
    "\n",
    "Año publicación: Año de publicación del libro.\n",
    "\n",
    "Sinopsis: Descripción o resumen del contenido del libro.\n",
    "\n",
    "Páginas: Cantidad de páginas.\n",
    "\n",
    "Revisión: Información editorial o revisión del texto.\n",
    "\n",
    "Idioma: Idioma del libro.\n",
    "\n",
    "Publicado: Estado o fecha de publicación.\n",
    "\n",
    "Estado: Estado del libro dentro del catálogo.\n",
    "\n",
    "Valoración: Valoración promedio de los usuarios.\n",
    "\n",
    "Nº Votos: Cantidad de votos recibidos por los usuarios.\n",
    "\n",
    "Enlace(s): Enlaces relacionados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a48b67",
   "metadata": {},
   "source": [
    "#### Filtrar las entradas del catálogo, de manera de quedarse solamente con los libros en idioma español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6dc835b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalog = df_catalog[df_catalog['Idioma'] == 'Español']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afe504",
   "metadata": {},
   "source": [
    "#### Limitar las entradas del catálogo a las que tenga su correspondiente libro digital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9b7ebbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de archivos EPUB disponibles: 8958\n"
     ]
    }
   ],
   "source": [
    "path_books = 'compressed/compressed'\n",
    "\n",
    "# Se crea una lista con los epubs en path_books\n",
    "epubs = [f for f in os.listdir(path_books) if f.endswith(\".epub\")]\n",
    "\n",
    "print(f\"Total de archivos EPUB disponibles: {len(epubs)}\")\n",
    "\n",
    "# Se crea un conjunto de ids válidos, es decir, se cargan los ids de los epubs que existen en la ruta path_books\n",
    "valids_id = set()\n",
    "for f in epubs:\n",
    "    try:\n",
    "        valids_id.add(int(f.replace(\".epub\", \"\")))\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "# Se reduce el catálogo a aquellos libros que tienen su versión digital (es decir, que existen en la carpeta descomprimida)\n",
    "df_catalog = df_catalog[df_catalog['EPL Id'].isin(valids_id)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5468d4",
   "metadata": {},
   "source": [
    "# Definición de las clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8ad0a",
   "metadata": {},
   "source": [
    "#### Analizar la distribución de libros por categoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8e2db753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Drama         1201\n",
       "Otros         1059\n",
       "Aventuras     1000\n",
       "Policial       998\n",
       "Realista       854\n",
       "Intriga        689\n",
       "Histórico      591\n",
       "Filosofía      553\n",
       "Historia       533\n",
       "Fantástico     470\n",
       "dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se separan los géneros y se eliminan los espacios despues de las comas y los posteriores al último caracter de la palabra\n",
    "all_genres = df_catalog['Géneros'].dropna()\\\n",
    "    .str.split(r',\\s*')\\\n",
    "    .apply(lambda lst: [g.strip() for g in lst])\n",
    "\n",
    "# Se cuentan los géneros, utilizando una doble compresión.\n",
    "# se toma cada sublista (es decir, cada lista de géneros para cada uno de los libros) en all_genres y a cada una de ellas, se les toma cada uno de los géneros\n",
    "# y se aplanan en una lista, donde Counter cuenta la cantidad de ocurrencias de cada uno de los géneros en la lista total \n",
    "genre_counter = Counter([g for sublist in all_genres for g in sublist])\n",
    "\n",
    "# Se crea un Series de pandas y se ordenan las ocurrencias de cada género, de manera descendente\n",
    "genre_proportion = pd.Series(genre_counter).sort_values(ascending=False)\n",
    "\n",
    "genre_proportion.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b3cf2",
   "metadata": {},
   "source": [
    "Se observa que las clases se encuentran desbalanceadas, predominan las primeras 4 clases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e53b67",
   "metadata": {},
   "source": [
    "#### Eliminar el género Otros por ser una categoría redundante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "483d2e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan los libros con únicamente el género \"Otros\"\n",
    "df_catalog = df_catalog[~df_catalog['Géneros'].str.fullmatch(r'Otros', na=False)]\n",
    "\n",
    "# Se elimina el género \"Otros\" de los demás libros, considerando nuevamente los casos con espacios post comas y post ultimo caracter\n",
    "# además del caso en que \"Otros\" es el primer género de la lista\n",
    "w = r'^\\s*Otros\\s*,?|,?\\s*Otros\\s*' #Patrón para todos los casos\n",
    "\n",
    "df_catalog['Géneros'] = (\n",
    "    df_catalog['Géneros']\n",
    "    .str.replace(w, '', regex=True)\n",
    "    .str.strip(', ')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9fa072",
   "metadata": {},
   "source": [
    "#### Proponer y justificar un criterio para elegir un único género cuando un libro tenga varios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "76efc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define una función para seleccionar el género con menor aparición global para cada libro\n",
    "# Se puede definir una cantidad mínima de apariciones globales del género, en caso de que ninguno cumpla eso, se selecciona\n",
    "# el de menor frecuencia\n",
    "def pick_rarest_genre_min(genres, min_count = 10):\n",
    "\n",
    "    # Se verifica si genres es NaN\n",
    "    if pd.isna(genres):\n",
    "        return None\n",
    "    \n",
    "    # Se divide la lista de géneros usando las comas como separador.\n",
    "    # strip elimina los espacios al inicio y al final\n",
    "    genre_list = [g.strip() for g in genres.split(',')]\n",
    "\n",
    "    # Se filtran los géneros que cumplan el mínimo de apariciones globales\n",
    "    # Si no tiene apariciones, considera 0. Toma como umbral aquellos con al menos min_count apariciones\n",
    "    filtered = [g for g in genre_list if genre_counter.get(g, 0) >= min_count]\n",
    "\n",
    "    # Se seleccionan aquellos géneros segun mínimas apariciones o que cumplan el umbral\n",
    "    if filtered:\n",
    "        # Se toma  menos frecuente entre los géneros que cumplen el mínimo de apariciones globales\n",
    "        rarest = min(filtered, key=lambda g: genre_counter.get(g, 0))\n",
    "    else:\n",
    "        # Si ninguno cumple el mínimo, se toma el género menos frecuente de la lista\n",
    "        rarest = min(genre_list, key=lambda g: genre_counter.get(g, 0))\n",
    "    \n",
    "    return rarest\n",
    "\n",
    "# Se crea una columna con el género con la menor cantidad de apariciones en el dataframe para cada libro\n",
    "df_catalog['Género_único'] = df_catalog['Géneros'].apply(pick_rarest_genre_min)\n",
    "\n",
    "# Se elimina la columna \"Géneros\" del catálogo\n",
    "df_catalog.drop(columns=['Géneros'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b32c57",
   "metadata": {},
   "source": [
    "#### Reportar la distribución final de libros por categoría"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b544e07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Género_único\n",
       "Realista           694\n",
       "Drama              662\n",
       "Policial           623\n",
       "Aventuras          606\n",
       "Intriga            596\n",
       "Histórico          477\n",
       "Historia           387\n",
       "Filosofía          364\n",
       "Fantástico         284\n",
       "Ciencia ficción    284\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se cuenta la cantidad de apariciones de cada género en la columna de 'Genero_unico'\n",
    "final_distribution = df_catalog['Género_único'].value_counts()\n",
    "\n",
    "final_distribution = final_distribution\n",
    "\n",
    "final_distribution.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec22546",
   "metadata": {},
   "source": [
    "Los géneros se encuentran más balanceados, por lo que no hay algunos que predominen demasiado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83505b75",
   "metadata": {},
   "source": [
    "#### Separar los libros para definir conjuntos de entrenamiento y testeo utilizando las proporciones 75/25. Fijar la semilla para reproducibilidad utilizando su número de padrón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ca20128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del catálogo completo: (8163, 17)\n",
      "Tamaño del conjunto de entrenamiento: (6122, 17)\n",
      "Tamaño del conjunto de testeo: (2041, 17)\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los géneros únicos\n",
    "unique_genre = df_catalog['Género_único'].unique()\n",
    "\n",
    "# Se genera un diccionario con una etiqueta para cada genero (1 : unique_genre +1)\n",
    "dict_genre = {genero: idx + 1 for idx, genero in enumerate(unique_genre)}\n",
    "\n",
    "# Se cargan las etiquetas para cada género en el df\n",
    "df_catalog['Etiqueta_género'] = df_catalog['Género_único'].map(dict_genre)\n",
    "\n",
    "# Se cargan en X los datos del catálogo, son seleccionados sin incluir las etiquetas ni su género\n",
    "X = df_catalog\n",
    "\n",
    "# Se cargan en y los datos a predecir\n",
    "y = df_catalog[\"Etiqueta_género\"]\n",
    "\n",
    "# Número de padrón\n",
    "student_id = 104241\n",
    "\n",
    "# Se separan en datos de entrenamiento y testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    random_state=student_id,\n",
    ")\n",
    "\n",
    "print('Tamaño del catálogo completo:', df_catalog.shape)\n",
    "print('Tamaño del conjunto de entrenamiento:', X_train.shape)\n",
    "print('Tamaño del conjunto de testeo:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24050d",
   "metadata": {},
   "source": [
    "# Preprocesamiento de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426b651",
   "metadata": {},
   "source": [
    "#### Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "00bc6e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_books = 'compressed/compressed'\n",
    "\n",
    "def get_text_epub(epub_path):\n",
    "\n",
    "    try:\n",
    "        # Se abre el archivo epub\n",
    "        book = epub.read_epub(epub_path)\n",
    "        \n",
    "        chapters_list = []\n",
    "\n",
    "        # Se recorren todos los documentos XHTML del libro\n",
    "        for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "            \n",
    "            # Se obtiene el contenido del documento XHTML\n",
    "            xhtml_content = item.get_content()\n",
    "            \n",
    "            # Se usa BeautifulSoup para limpiar el xhtml y solo obtener el texto\n",
    "            soup = BeautifulSoup(xhtml_content, 'html.parser')\n",
    "            \n",
    "            # Se limpia el texto extraído\n",
    "            clean_text = soup.get_text()\n",
    "            \n",
    "            chapters_list.append(clean_text)\n",
    "        \n",
    "        # Se unifican los capitulos en un string\n",
    "        return \" \".join(chapters_list)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"No se encontró el archivo: {epub_path}\")\n",
    "        return None\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando (corrupto): {epub_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def get_text_epub_id (id_book):\n",
    "    \n",
    "    # Se construye la ruta completa al libro epub\n",
    "    filename = f\"{id_book}.epub\"\n",
    "    \n",
    "    epub_path = os.path.join(path_books, filename)\n",
    "\n",
    "    return get_text_epub(epub_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8cd575",
   "metadata": {},
   "source": [
    "#### El formato de libros epub es un archivo comprimido zip que contiene la metadata y estructura del libro, archivos multimedia y archivos xhtml con el texto del libro. Extraer el texto de esos archivos. Podrá realizarlo manualmente o valerse de bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "99c820d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libros en X_train: 6122\n",
      "Libros en X_test: 2041\n",
      "\n",
      "\n",
      "Error procesando (corrupto): compressed/compressed\\58986.epub: \"There is no item named 'OEBPS/Text/autor.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\40832.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\19881.epub: \"There is no item named 'OEBPS/toc.ncx' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\29638.epub: \"There is no item named 'OEBPS/Text/II_elreformatorio.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\71931.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\27671.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\3201.epub: \"There is no item named 'OEBPS/toc.ncx' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\7517.epub: \"There is no item named 'OEBPS/Text/Chapter1.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\53324.epub: \"There is no item named 'OEBPS/Text/epl03.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\17654.epub: \"There is no item named 'OEBPS/toc.ncx' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\64292.epub: \"There is no item named 'OEBPS/Text/autor.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\49562.epub: \"There is no item named 'OEBPS/Text/Cap01.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\38390.epub: \"There is no item named 'OEBPS/Fonts/bakery.ttf' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\38383.epub: \"There is no item named 'OEBPS/Fonts/bakery.ttf' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\19231.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\25312.epub: \"There is no item named 'OEBPS/toc.ncx' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\58570.epub: \"There is no item named 'OEBPS/Text/autor.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\38350.epub: \"There is no item named 'OEBPS/Fonts/bakery.ttf' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\49935.epub: \"There is no item named 'OEBPS/Text/TOC.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\24431.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\47415.epub: \"There is no item named 'OEBPS/Text/Capitulo01.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\42958.epub: \"There is no item named 'OEBPS/Text/cubierta.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\10045.epub: \"There is no item named 'OEBPS/Text/autor.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\54762.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\13872.epub: \"There is no item named 'OEBPS/toc.ncx' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\72900.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\26917.epub: \"There is no item named 'OEBPS/toc.ncx' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\42080.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\45806.epub: \"There is no item named 'OEBPS/Text/Acto01.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\26704.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\48196.epub: \"There is no item named 'OEBPS/Text/autor.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\72317.epub: \"There is no item named 'OEBPS/Text/autor.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\27510.epub: \"There is no item named 'OEBPS/Text/Cap03.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\50972.epub: \"There is no item named 'OEBPS/Images/EPL_logo.png' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\38432.epub: \"There is no item named 'OEBPS/Fonts/bakery.ttf' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\34939.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\61431.epub: \"There is no item named 'OEBPS/Text/Carta01.xhtml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\56207.epub: \"There is no item named 'META-INF/container.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\15162.epub: \"There is no item named 'OEBPS/Misc/com.apple.ibooks.display-options.xml' in the archive\"\n",
      "Error procesando (corrupto): compressed/compressed\\49499.epub: \"There is no item named 'OEBPS/Images/EPL_logo.png' in the archive\"\n",
      "\n",
      "\n",
      "Se completó la extracción de los textos\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Datos válidos para X_train: (6094, 18)\n",
      "Datos válidos para X_test: (2029, 18)\n",
      "\n",
      "\n",
      "Datos válidos para y_train: (6094,)\n",
      "Datos válidos para y_test: (2029,)\n"
     ]
    }
   ],
   "source": [
    "# n_samples = 100\n",
    "\n",
    "# # Se toman unas pocas muestras para evitar ejecutar con todos los archivos juntos. Se toman por índices para conservar alineados los datos.\n",
    "# X_train = X_train.iloc[:n_samples].copy()\n",
    "# y_train = y_train.iloc[:n_samples].copy()\n",
    "# X_test = X_test.iloc[:n_samples].copy()\n",
    "# y_test = y_test.iloc[:n_samples].copy()\n",
    "\n",
    "print(f\"Libros en X_train: {len(X_train)}\")\n",
    "print(f\"Libros en X_test: {len(X_test)}\\n\\n\")\n",
    "\n",
    "\n",
    "# Se cargan en X_train y X_test los textos de cada uno de los libros, utilizando su ID\n",
    "X_train.loc[:, 'texto'] = X_train['EPL Id'].apply(get_text_epub_id)\n",
    "X_test.loc[:, 'texto'] = X_test['EPL Id'].apply(get_text_epub_id)\n",
    "\n",
    "print(\"\\n\\nSe completó la extracción de los textos\\n\\n\")\n",
    "\n",
    "# Se eliminan las filas que no tienen texto (vacías o con NaN)\n",
    "X_train_clean = X_train.dropna(subset=['texto'])\n",
    "X_test_clean = X_test.dropna(subset=['texto'])\n",
    "\n",
    "# Se seleccionan las etiquetas que corresponden a libros cuyo texto no está vacío\n",
    "y_train_clean = y_train.loc[X_train_clean.index]\n",
    "y_test_clean = y_test.loc[X_test_clean.index]\n",
    "\n",
    "print('\\n\\nDatos válidos para X_train:', X_train_clean.shape)\n",
    "print('Datos válidos para X_test:', X_test_clean.shape)\n",
    "\n",
    "print('\\n\\nDatos válidos para y_train:', y_train_clean.shape)\n",
    "print('Datos válidos para y_test:', y_test_clean.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdb1d0",
   "metadata": {},
   "source": [
    "#### Aplicar CountVectorizer (sklearn) al texto. Ajustar max_df, min_df y stop_words a criterio personal, justificando las decisiones. El corpus de texto completo es demasiado extenso para la memoria. Se sugiere el uso de Generators para procesar el texto plano on-demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ee14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\solek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Se declara el generator, para procesar el texto por partes\n",
    "def text_generator(df_clean):\n",
    "    for texto in df_clean['texto']:\n",
    "        yield texto\n",
    "\n",
    "# Se descargan las stopwords, (las palabras que ignora por ser comunes a prácticamente todos los textos, tales como conectores)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Se definen las stopwords en español\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# Se crea el vectorizador utilizando las stopwords en español (Palabras que se ignoran del texto completo).\n",
    "# Se cuentan las apariciones de las palabras en los textos, \n",
    "# siendo guardados en una matriz de shape: (n_libros, n_palabras_del_vocabulario) cada posición, almacena esa cantidad de apariciones\n",
    "# Se seleccionan los porcentajes para min_df y max_df ya que se considera que palabras demasiado\n",
    "# poco frecuentes o demasiado frecuentes en el total de los libros, no aportan información significativa para distinguir entre\n",
    "# un género y otro.\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=spanish_stopwords,\n",
    "    max_df=0.9,    # Se descartan las palabras más frecuentes, las que aparecen en más del 90% de los libros\n",
    "    min_df= 0.01   # Se descartan las palabras que aparecen en menos del 1% de los libros\n",
    ")\n",
    "\n",
    "\n",
    "# Se entrena el vectorizador con todos los textos de los libros, utilizando los generators para evitar cargar el texto completo\n",
    "train_gen = list(text_generator(X_train_clean))\n",
    "X_train_vec = vectorizer.fit_transform(train_gen)\n",
    "\n",
    "test_gen = list(text_generator(X_test_clean))\n",
    "X_test_vec = vectorizer.transform(test_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f78656",
   "metadata": {},
   "source": [
    "#### Se desea comprobar el funcionamiento del vectorizador. Para ello, aplicar el vectorizador ya entrenado a dos obras clásicas del catálogo de diferentes géneros (por ejemplo, “Estudio en escarlata” y “Orgullo y prejuicio”). Descartar las palabras presentes en ambos libros. Luego, para cada libro, reportar las 40 palabras más frecuentes. Interpretar los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026a7f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más frecuentes en 'Alicia en el país de las maravillas' (excluyendo las que se comparten):\n",
      "alicia -> 433\n",
      "tortuga -> 64\n",
      "grifo -> 55\n",
      "conejo -> 53\n",
      "lirón -> 39\n",
      "gustaría -> 20\n",
      "bebé -> 14\n",
      "corten -> 13\n",
      "cocinera -> 13\n",
      "mantequilla -> 11\n",
      "enseguida -> 10\n",
      "jardineros -> 10\n",
      "croquet -> 10\n",
      "moraleja -> 10\n",
      "cheshire -> 9\n",
      "sonrisa -> 8\n",
      "pizarras -> 8\n",
      "melaza -> 8\n",
      "cogió -> 8\n",
      "centímetros -> 7\n",
      "pizca -> 7\n",
      "temblorosa -> 7\n",
      "lagartija -> 7\n",
      "nadar -> 7\n",
      "ansiedad -> 7\n",
      "debería -> 6\n",
      "cerdito -> 6\n",
      "chilló -> 6\n",
      "señorita -> 6\n",
      "carroll -> 6\n",
      "enfadado -> 6\n",
      "barbilla -> 6\n",
      "puertecita -> 5\n",
      "pipa -> 5\n",
      "cambiado -> 5\n",
      "dedal -> 5\n",
      "gruñido -> 5\n",
      "medía -> 5\n",
      "intrascendente -> 5\n",
      "lío -> 5\n",
      "\n",
      "Palabras más frecuentes en 'Historia de los heterodoxos españoles' (excluyendo las que se comparten):\n",
      "et -> 2822\n",
      "san -> 1633\n",
      "españa -> 1510\n",
      "juan -> 1226\n",
      "iglesia -> 1197\n",
      "siglo -> 1033\n",
      "non -> 846\n",
      "fe -> 801\n",
      "fr -> 786\n",
      "doctrina -> 740\n",
      "obispo -> 720\n",
      "espíritu -> 689\n",
      "santo -> 685\n",
      "pedro -> 679\n",
      "etc -> 645\n",
      "ad -> 625\n",
      "madrid -> 620\n",
      "inquisición -> 527\n",
      "filosofía -> 521\n",
      "cf -> 519\n",
      "españoles -> 503\n",
      "valdés -> 503\n",
      "qui -> 502\n",
      "quod -> 485\n",
      "cristo -> 474\n",
      "erasmo -> 473\n",
      "ut -> 465\n",
      "religión -> 460\n",
      "páginas -> 453\n",
      "ii -> 436\n",
      "biblioteca -> 432\n",
      "obispos -> 432\n",
      "ciencia -> 426\n",
      "francisco -> 424\n",
      "concilio -> 416\n",
      "sevilla -> 409\n",
      "per -> 408\n",
      "ley -> 397\n",
      "antonio -> 396\n",
      "carlos -> 395\n",
      "Entre los tops de palabras más frecuentes, se comparten:\n",
      "\n",
      " set()\n"
     ]
    }
   ],
   "source": [
    "# Se extraen los textos de 2 libros diferentes\n",
    "text_alice = X_train_clean.loc[X_train_clean['Título'] == 'Alicia en el país de las maravillas (il. de Marta Gómez-Pintado)', 'texto'].iloc[0]\n",
    "text_spain   = X_train_clean.loc[X_train_clean['Título'] == 'Historia de los heterodoxos españoles', 'texto'].iloc[0]\n",
    "\n",
    "\n",
    "# Se vectorizan los textos de cada libro (shapes: 1xn_vocabulario)\n",
    "X_alice = vectorizer.transform([text_alice])\n",
    "X_spain   = vectorizer.transform([text_spain])\n",
    "\n",
    "\n",
    "freq_alice = np.squeeze(np.asarray(X_alice.toarray()))\n",
    "freq_spain   = np.squeeze(np.asarray(X_spain.toarray()))\n",
    "\n",
    "# Se obtienen los índices donde realmente hay palabras\n",
    "idx_alice = set(np.where(freq_alice > 0)[0])\n",
    "idx_spain   = set(np.where(freq_spain > 0)[0])\n",
    "\n",
    "# Se obtienen las palabras en común\n",
    "common_words = idx_alice.intersection(idx_spain)\n",
    "\n",
    "# Se crean listas de las palabras únicas en cada libro\n",
    "words_alice_only = list(idx_alice - common_words)\n",
    "words_spain_only   = list(idx_spain - common_words)\n",
    "\n",
    "# Se obtiene el vocabulario en el vectorizador\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Se obtiene la cantidad de apariciones de cada palabra en cada libro\n",
    "freq_alice_filtered = [(vocabulary[i], freq_alice[i]) for i in words_alice_only]\n",
    "top_alice = sorted(freq_alice_filtered, key=lambda x: x[1], reverse=True)[:40]\n",
    "\n",
    "freq_spain_filtered = [(vocabulary[i], freq_spain[i]) for i in words_spain_only]\n",
    "top_spain = sorted(freq_spain_filtered, key=lambda x: x[1], reverse=True)[:40]\n",
    "\n",
    "\n",
    "print(\"Palabras más frecuentes en 'Alicia en el país de las maravillas' (excluyendo las que se comparten):\")\n",
    "for word, freq in top_alice:\n",
    "        print(word, '->', freq)\n",
    "\n",
    "print(\"\\nPalabras más frecuentes en 'Historia de los heterodoxos españoles' (excluyendo las que se comparten):\")\n",
    "for word, freq in top_spain:\n",
    "        print(word, '->', freq)\n",
    "\n",
    "\n",
    "# shared = set(top_alice).intersection(top_spain)\n",
    "# print('Entre los tops de palabras más frecuentes, se comparten:\\n\\n', shared)\n",
    "\n",
    "# print(X_alice.shape)\n",
    "# print(X_spain.shape)\n",
    "# print(vocabulary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bcb862",
   "metadata": {},
   "source": [
    "Dado que se eligieron 2 libros de temáticas muy diferentes, es esperable que ninguno de los libros compartan palabras en su top de 40 palabras más usadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7e81e7",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7a5af",
   "metadata": {},
   "source": [
    "#### Sobre Naive Bayes (como clasificador): \n",
    "\n",
    "Es un clasificador probabilístico, que busca estimar la probabilidad de que un dato $x$ pertenezca a una clase $y$ y clasifica según qué clase tiene mayor probabilidad.\n",
    "\n",
    "En este caso, los datos son el texto de los libros, mientras que las clases, son los géneros de cada libro. Entonces, se busca que, dado un texto de un libro, se pueda predecir a qué género pertenece.\n",
    "\n",
    "#### Sobre la hipótesis Naive:\n",
    "\n",
    "La hipótesis Naive se basa en asumir que todas las palabras son independientes entre sí, dado que ya conocemos la clase de cada una.\n",
    "Si, por ejemplo, tenemos el libro \"Alicia en el país de las maravillas\", la aparición de \"conejo\" y \"alicia\" es independiente entre sí dado el género.\n",
    "Lo cual, no es estrictamente cierto.\n",
    "\n",
    "#### Sobre Naive Bayes Multinomial:\n",
    "\n",
    "En este modelo, la probabilidad asignada a cada clase (género del libro), dado que se tiene la muestra x (el texto del libro en este caso), es:\n",
    "\n",
    "$$\n",
    "p(y \\mid x) \\propto p(y) \\prod_{i=1}^{n} p(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "Esto nace del teorema de Bayes:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y) \\, P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "$y$ es la clase, en este caso el género del libro.\n",
    "\n",
    "x es el texto de un libro.\n",
    "\n",
    "$P(y \\mid x)$ Es la probabilidad de que el libro sea de la clase $y$ dado el texto del libro (lo que se busca predecir).\n",
    "\n",
    "$P(y)$ Es la probabilidad a priori de la clase $y$ (en este caso, la proporción de libros de cada género).\n",
    "\n",
    "$P(x)$ Es la probabilidad marginal de observar esas palabras.\n",
    "\n",
    "$P(x \\mid y)$ Es la probabilidad de observar las palabras x, dado que el libro es del género $y$.\n",
    "\n",
    "La hipótesis Naive, entra a la hora de calcular $P(x \\mid y)$, ya que, dado que las palabras $x_i$ se consideran independientes dado el género al que pertenece el texto del libro, entonces:\n",
    "\n",
    "$$\n",
    "P(x \\mid y) = \\prod_{i=1}^{n} P(x_i \\mid y)\n",
    "$$\n",
    "\n",
    "\n",
    "Para el caso de la multinomial, esta probabilidad (sin considerar la marginal de x) puede escribirse como:\n",
    "\n",
    "$$\n",
    "p( y = k | x ) \\propto c_y \\cdot \\prod_{j=1}^{V}(\\theta_{j}^{(k)})^{N_j}\n",
    "$$ \n",
    "\n",
    "Donde:\n",
    "\n",
    "$y$ es la clase, en este caso el género del libro.\n",
    "\n",
    "x es el texto de un libro, que contiene d palabras: x=($x_1$, $x_2$, ... , $x_d$).\n",
    "\n",
    "$c_y$ es la probabilidad a priori de cada clase (es decir, la cantidad de líbros de un género con respecto al total de libros).\n",
    "\n",
    "$\\theta_j$ son las probabilidades de que se encuentre en el texto la palabra $j$ ($j$ va desde 1 hasta $V$, es decir, abarca todo el vocabulario), dado el género del libro es $y$\n",
    "\n",
    "$N_j$ es la cantidad de veces que aparece la palabra j en el libro.\n",
    "\n",
    "\n",
    "Luego, dado que las probabilidades pueden volverse muy pequeñas y causar inestabilidad numérica, se trabaja con el logaritmo de la probabilidad:\n",
    "\n",
    "$log (p( y = k | x )) = cte + log(c_y) +\\sum_{j=1}^{V}(N_j \\cdot log(\\theta_{j}^{k})) $\n",
    "\n",
    "\n",
    "\n",
    "Entrenamiento del modelo:\n",
    "\n",
    "Para el entrenamiento del modelo, se comienza calculando las probabilidades a priori de cada clase, es decir, la cantidad de libros cada género con respecto al total de libros:\n",
    "\n",
    "$$\n",
    "c_k = p(y=k) = \\frac{\\#\\{y_i=k\\}}{n}\n",
    "$$\n",
    "\n",
    "donde:\n",
    "\n",
    "$k$ es la clase o género\n",
    "\n",
    "$i$ es el i-ésimo libro \n",
    "\n",
    "$y_i$ es el género del i-ésimo libro\n",
    "\n",
    "$n$ es el total de libros\n",
    "\n",
    "luego, se calcula el logaritmo de esta probabilidad, para preparar el cálculo de predict_proba\n",
    "\n",
    "el paso siguiente, consiste en calcular $\\theta$ :\n",
    "\n",
    "$$\n",
    "\\theta^{k}_{j}\n",
    "=\n",
    "\\frac{N_{kj} + \\alpha_{j}}\n",
    "{\\sum_{m=1}^{V} (N_{km} + \\alpha_{m})}\n",
    "$$\n",
    "\n",
    "y el logaritmo del mismo, para preparar los cálculos de la predicción soft."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a46ea",
   "metadata": {},
   "source": [
    "#### Utilizando solamente numpy y scipy, implementar el clasificador MNB. El mismo debe contener los métodos fit, predict y predict_proba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5565e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNB:\n",
    "\n",
    "    def __init__(self, alpha):\n",
    "        self.classes = None\n",
    "        self.class_count = None\n",
    "        self.class_log_prior = None\n",
    "        self.words_per_genre = None\n",
    "        self.log_theta = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Entrenamiento\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # Se convierte la entrada en un array de numpy\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Se guardan las clases utilizando los identificadores de los géneros\n",
    "        self.classes = np.unique(y)\n",
    "\n",
    "        # Se guarda la cantidad de clases existentes (cantidad de géneros)\n",
    "        K = len(self.classes)\n",
    "\n",
    "        # Se carga N como al cantidad de libros que están vectorizados y V como la cantidad de palabras del vocabulario\n",
    "        N = X.shape[0]\n",
    "        V = X.shape[1]\n",
    "\n",
    "        # Se calculan cuantos libros pertenecen a cada género\n",
    "        self.class_count = np.array([(y == c).sum() for c in self.classes], dtype=float)\n",
    "\n",
    "        # Se calcula el logaritmo de la probabilidad a priori: log (# libros por género / # libros totales) (para evitar inestabilidad numérica)\n",
    "        self.class_log_prior = np.log(self.class_count / N)\n",
    "\n",
    "        # Se crea un vector de ceros de K x V, para contar cuantas veces aparece cada palabra dentro de cada clase\n",
    "        # Es decir, se cuenta cuantas veces aparece una palabra en un libro de un determinado género\n",
    "        self.words_per_genre = np.zeros((K, V), dtype=float)\n",
    "\n",
    "        # Se realiza la cuenta de las palabras presentes en cada género para todos los géneros    \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            self.words_per_genre[idx] = X[y == c].sum(axis=0)\n",
    "\n",
    "        # Se genera el vector de alphas: si es un escalar se llena el array de V alphas con el valor dado\n",
    "        # en caso de que sea una lista (o similares), se lo convierte a array\n",
    "        if np.isscalar(self.alpha):\n",
    "            alpha_vec = np.full(V, self.alpha)\n",
    "        else:\n",
    "            alpha_vec = np.asarray(self.alpha)\n",
    "\n",
    "        # Se calcula:  N_kj + alpha_j\n",
    "        numerator = self.words_per_genre + alpha_vec  # shape (K, V)\n",
    "\n",
    "        # Se calcula: sum(N_km + alpha_m) (m va entre 1 y V)\n",
    "        denominator = numerator.sum(axis=1, keepdims=True)  # (K, 1)\n",
    "\n",
    "        # Se calculan los thetas (probabilidades de que aparezcan  cada una de las palabras del vocabulario dada cada una de las clases)\n",
    "        theta_hat = numerator / denominator\n",
    "\n",
    "        # Se calculan las log-probabilidades para los thetas\n",
    "        self.log_theta = np.log(theta_hat)\n",
    "\n",
    "\n",
    "        # print('log_theta', self.log_theta.shape)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predicción soft\n",
    "    def predict_proba(self, X):\n",
    "\n",
    "        # Dado que cada posición de la vectorización del textos es: X[i, m] = N_m\n",
    "        # Entonces, se usa el producto matricial @ para el cálculo\n",
    "        # Además, se suma el logaritmo de la probabilidad a priori\n",
    "        # todo esto para calcular el logaritmo de la probabilidad de un libro pertenezca a un determinado género, dado el texto del mismo\n",
    "        log_prob = X @ self.log_theta.T + self.class_log_prior\n",
    "\n",
    "        # Se normaliza log_prob, evitando inestabilidad numérica\n",
    "        log_norm = logsumexp(log_prob, axis=1, keepdims=True)\n",
    "\n",
    "        # Se vuelve al espacio de probabilidades finalmente\n",
    "        return np.exp(log_prob - log_norm)\n",
    "\n",
    "    # Predicción hard\n",
    "    def predict(self, X):\n",
    "        return self.classes[np.argmax(self.predict_proba(X), axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dacce3",
   "metadata": {},
   "source": [
    " #### Reportar el Accuracy y el Macro F1, tanto para entrenamiento como testeo. ¿Cuál sería la probabilidad de error asociada a un clasificador dummy en esta tarea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9c87c211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas del MNB\n",
      "Accuracy (train): 75.714%\n",
      "Accuracy (test): 49.581%\n",
      "Macro F1 (train): 80.53%\n",
      "Macro F1 (test): 37.565%\n",
      "\n",
      "\n",
      "Métricas Dummy Classifier\n",
      "Accuracy (train): 8.582%\n",
      "Accuracy (test): 8.132%\n",
      "Macro F1 (train): 0.304%\n",
      "Macro F1 (test): 0.358%\n"
     ]
    }
   ],
   "source": [
    "model = MNB(alpha = 0.1)\n",
    "\n",
    "model.fit(X_train_vec,y_train_clean)\n",
    "\n",
    "# Se obtienen las predicciones del modelo\n",
    "y_pred_train = model.predict(X_train_vec)\n",
    "y_pred_test  = model.predict(X_test_vec)\n",
    "\n",
    "# Se obtienen los accuracy\n",
    "acc_train = accuracy_score(y_train_clean, y_pred_train)\n",
    "acc_test  = accuracy_score(y_test_clean, y_pred_test)\n",
    "\n",
    "# Se obtienen los Macro F1\n",
    "f1_train = f1_score(y_train_clean, y_pred_train, average='macro')\n",
    "f1_test  = f1_score(y_test_clean, y_pred_test, average='macro')\n",
    "\n",
    "print('Métricas del MNB')\n",
    "print(f'Accuracy (train): {round(acc_train *100,3)}%' )\n",
    "print(f'Accuracy (test): {round(acc_test *100,3)}%')\n",
    "print(f'Macro F1 (train): {round(f1_train*100,3)}%')\n",
    "print(f'Macro F1 (test): {round(f1_test*100,3)}%')\n",
    "\n",
    "####### clasificador dummy\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# Entrenamiento\n",
    "dummy.fit(X_train_vec, y_train_clean)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_train = dummy.predict(X_train_vec)\n",
    "y_pred_test  = dummy.predict(X_test_vec)\n",
    "\n",
    "# Métricas\n",
    "acc_train = accuracy_score(y_train_clean, y_pred_train)\n",
    "acc_test  = accuracy_score(y_test_clean, y_pred_test)\n",
    "f1_train = f1_score(y_train_clean, y_pred_train, average='macro')\n",
    "f1_test  = f1_score(y_test_clean, y_pred_test, average='macro')\n",
    "\n",
    "print('\\n\\nMétricas Dummy Classifier')\n",
    "print(f'Accuracy (train): {round(acc_train *100,3)}%' )\n",
    "print(f'Accuracy (test): {round(acc_test *100,3)}%')\n",
    "print(f'Macro F1 (train): {round(f1_train*100,3)}%')\n",
    "print(f'Macro F1 (test): {round(f1_test*100,3)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab3bfe",
   "metadata": {},
   "source": [
    "Se evidencia que el modelo realiza una mejor predicción que el clasificador dummy, el cual escoje el género más frecuente.\n",
    "\n",
    "Por otro lado, se puede notar que, si se analizan los datos de testeo, el clasificador MNB tiene las clases ligeramente desbalanceadas, ya que el accuracy y el macro-f1 no coinciden. El accuracy, mide los aciertos con respecto al total de predicciones, mientras que el macro-f1, combina cuantas predicciones de una clase eran correctas y de todos los casos reales (es decir, de todos los libros de una clase) cuales se predijeron correctamente. \n",
    "\n",
    "Por esta razón, es evidente que hay géneros que se predicen peor que otros, dado que se ve afectado por las clases con menos aciertos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0acd6",
   "metadata": {},
   "source": [
    "#### Se desea efectuar un análisis cualitativo de los errores de clasificación. Para ello, seleccione las 10 obras más populares (de testeo) que hayan sido clasificadas incorrectamente. Interpretar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2617de7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Título</th>\n",
       "      <th>Género real</th>\n",
       "      <th>Género predicho</th>\n",
       "      <th>Valoración</th>\n",
       "      <th>Nº Votos</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>1984</td>\n",
       "      <td>Ciencia ficción</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1021</td>\n",
       "      <td>9086.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26330</th>\n",
       "      <td>El Hobbit</td>\n",
       "      <td>Fantástico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.9</td>\n",
       "      <td>895</td>\n",
       "      <td>7965.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17099</th>\n",
       "      <td>Un mundo feliz (trad. Ramón Hernández)</td>\n",
       "      <td>Filosófico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.4</td>\n",
       "      <td>625</td>\n",
       "      <td>5250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38568</th>\n",
       "      <td>El conde de Montecristo</td>\n",
       "      <td>Aventuras</td>\n",
       "      <td>Realista</td>\n",
       "      <td>9.3</td>\n",
       "      <td>397</td>\n",
       "      <td>3692.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27191</th>\n",
       "      <td>El retrato de Dorian Gray</td>\n",
       "      <td>Terror</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.6</td>\n",
       "      <td>357</td>\n",
       "      <td>3070.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9883</th>\n",
       "      <td>El extranjero</td>\n",
       "      <td>Filosófico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.4</td>\n",
       "      <td>312</td>\n",
       "      <td>2620.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Soy leyenda</td>\n",
       "      <td>Terror</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.5</td>\n",
       "      <td>294</td>\n",
       "      <td>2499.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>Don Quijote de la Mancha (IV CENTENARIO)</td>\n",
       "      <td>Aventuras</td>\n",
       "      <td>Realista</td>\n",
       "      <td>9.4</td>\n",
       "      <td>256</td>\n",
       "      <td>2406.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11617</th>\n",
       "      <td>El lobo estepario</td>\n",
       "      <td>Filosófico</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.7</td>\n",
       "      <td>271</td>\n",
       "      <td>2357.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46891</th>\n",
       "      <td>La isla del tesoro</td>\n",
       "      <td>Aventuras</td>\n",
       "      <td>Realista</td>\n",
       "      <td>8.9</td>\n",
       "      <td>258</td>\n",
       "      <td>2296.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Título      Género real  \\\n",
       "12092                                      1984  Ciencia ficción   \n",
       "26330                                 El Hobbit       Fantástico   \n",
       "17099    Un mundo feliz (trad. Ramón Hernández)       Filosófico   \n",
       "38568                   El conde de Montecristo        Aventuras   \n",
       "27191                 El retrato de Dorian Gray           Terror   \n",
       "9883                              El extranjero       Filosófico   \n",
       "994                                 Soy leyenda           Terror   \n",
       "1584   Don Quijote de la Mancha (IV CENTENARIO)        Aventuras   \n",
       "11617                         El lobo estepario       Filosófico   \n",
       "46891                        La isla del tesoro        Aventuras   \n",
       "\n",
       "      Género predicho  Valoración  Nº Votos   score  \n",
       "12092        Realista         8.9      1021  9086.9  \n",
       "26330        Realista         8.9       895  7965.5  \n",
       "17099        Realista         8.4       625  5250.0  \n",
       "38568        Realista         9.3       397  3692.1  \n",
       "27191        Realista         8.6       357  3070.2  \n",
       "9883         Realista         8.4       312  2620.8  \n",
       "994          Realista         8.5       294  2499.0  \n",
       "1584         Realista         9.4       256  2406.4  \n",
       "11617        Realista         8.7       271  2357.7  \n",
       "46891        Realista         8.9       258  2296.2  "
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se genera un dataset de testeo para trabajar cómodamente\n",
    "df_test_clean = X_test_clean.copy()\n",
    "df_test_clean[\"y_true\"] = y_test_clean\n",
    "df_test_clean[\"y_pred_test\"] = y_pred_test\n",
    "\n",
    "# Se añade la columna de texto del género\n",
    "df_test_clean[\"Género real\"] = df_test_clean[\"Género_único\"] \n",
    "df_test_clean[\"Género predicho\"] = df_test_clean[\"y_pred_test\"].map({v:k for k,v in dict_genre.items()})\n",
    "\n",
    "# Se crea una columna con booleanos, para determinar si la predicción fue correcta (True = coinciden)\n",
    "df_test_clean[\"es_correcta\"] = (df_test_clean[\"y_true\"] == df_test_clean[\"y_pred_test\"])\n",
    "\n",
    "# Se seleccionan solo los errores\n",
    "errors = df_test_clean[df_test_clean[\"es_correcta\"] == False].copy()\n",
    "\n",
    "# Se crea una columna con el score, considerando la cantidad de votos y la valoración promedio\n",
    "errors[\"score\"] = errors[\"Valoración\"] * errors[\"Nº Votos\"]\n",
    "\n",
    "\n",
    "# Se ordena por este score de mayor a menor y tomar los del top 10\n",
    "errors_rank10 = errors.sort_values(\"score\", ascending=False).head(10)\n",
    "\n",
    "# Se muestra el top10 de los errores más valorado\n",
    "errors_rank10[[\"Título\", \"Género real\", \"Género predicho\", \"Valoración\", \"Nº Votos\", \"score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e633446",
   "metadata": {},
   "source": [
    "El modelo, no falla en identificar los elementos Realistas de estos libros, por ejemplo, para el caso de \"El retrato de Dorian Gray\", el modelo identifica los matices realistas del mismo, sin embargo, no logra identificar los elementos fantásticos del libro (las deformaciones del retrato) por lo que termina realizando una predicción en base al vocabulario común en el mismo.\n",
    "Algo similar ocurre con \"El Hobbit\" que utiliza mucho vocabulario realista, haciendo que las palabras más importantes para este género, como la aparición de animales fantásticos, queden opacadas, siendo poco relevantes para la determinación del género, cuando deberían ser todo lo contrario.\n",
    "\n",
    "Esto se debe a que el modelo utiliza las palabras más frecuentes para seleccionar el género y al ser también entrenado con libros cuya mayoría son realistas, el mismo \"entiende\" que predecir que el género es realista, es lo más común cuando encuentra palabras de este género con mucha frecuencia, todo esto en base a los datos de entrenamiento. Si bien esto último tiene poco peso para este caso, dado que los géneros se encuentran bastante balanceados como se vió en la distribución final de géneros más arriba.\n",
    "\n",
    "En definitiva, el modelo identifica eficazmente el vocabulario general del libro, el más frecuente, pero falla en darle más importancia a palabras que de alguna manera ayudan a determinar con precisión el género del libro. Por ejemplo, si se asignaran pesos más altos a las palabras menos frecuentes, como darle un peso alto a la palabra \"dragón\" en \"El Hobbit\", ayudaría a determinar que el libro es fantástico.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
